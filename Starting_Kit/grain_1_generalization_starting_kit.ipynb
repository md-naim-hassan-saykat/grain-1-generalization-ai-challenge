{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f09168ba-aee6-499d-920f-9a579fb2e51f",
      "metadata": {
        "id": "f09168ba-aee6-499d-920f-9a579fb2e51f"
      },
      "source": [
        "# Grain Variety Classification – Generalization Track\n",
        "\n",
        "**Course:** M1 Artificial Intelligence – AI Challenge  \n",
        "**Group:** Group 1 – Grain (Generalization)  \n",
        "**Institution:** Université Paris-Saclay  \n",
        "\n",
        "## Objective\n",
        "This notebook provides a **baseline starting kit** for the Grain 1 generalization task.\n",
        "It demonstrates dataset loading, a simple baseline model, and an evaluation pipeline.\n",
        "\n",
        "This baseline is intentionally simple and serves as a reference for further improvements."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36cd7134-67aa-4aab-b287-26df611b5755",
      "metadata": {
        "id": "36cd7134-67aa-4aab-b287-26df611b5755"
      },
      "source": [
        "The boolean variable COLAB is used to detect whether the notebook is executed on Google Colab, enabling environment-specific setup when needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "2f9a41e1-ae51-4dda-83d2-ba725991c793",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f9a41e1-ae51-4dda-83d2-ba725991c793",
        "outputId": "7abbfe2e-e743-4170-c7ed-9d493ac63ebe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'grain-1-generalization-ai-challenge'...\n",
            "remote: Enumerating objects: 15, done.\u001b[K\n",
            "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
            "remote: Total 15 (delta 0), reused 9 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (15/15), 223.07 KiB | 1.66 MiB/s, done.\n",
            "/content/grain-1-generalization-ai-challenge/Starting_Kit\n"
          ]
        }
      ],
      "source": [
        "# Detect whether we are running on Google Colab\n",
        "COLAB = \"google.colab\" in str(get_ipython())\n",
        "\n",
        "if COLAB:\n",
        "    !git clone --depth 1 https://github.com/md-naim-hassan-saykat/grain-1-generalization-ai-challenge.git\n",
        "    %cd grain-1-generalization-ai-challenge/Starting_Kit"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30e0f37b-38ce-4471-bd28-175323314b9b",
      "metadata": {
        "id": "30e0f37b-38ce-4471-bd28-175323314b9b"
      },
      "source": [
        "# 0 - Imports & Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "0a956880-e2b9-4868-8446-ad66302d9211",
      "metadata": {
        "id": "0a956880-e2b9-4868-8446-ad66302d9211"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import json\n",
        "import zipfile\n",
        "import datetime\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7ebf1d0",
      "metadata": {
        "id": "d7ebf1d0"
      },
      "source": [
        "# 1 - Data\n",
        "This section handles dataset loading and basic preprocessing for the Grain 1\n",
        "generalization challenge.\n",
        "\n",
        "We assume the dataset is organized as:\n",
        "data/\n",
        " ├── train/\n",
        " │    ├── class_1/\n",
        " │    ├── class_2/\n",
        " │    └── ...\n",
        " ├── val/\n",
        " └── test/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "e2e3a3d1",
      "metadata": {
        "id": "e2e3a3d1"
      },
      "outputs": [],
      "source": [
        "class Data:\n",
        "    \"\"\"\n",
        "    Data loader class for the Grain 1 Generalization challenge.\n",
        "    Expected folder structure:\n",
        "      data_dir/\n",
        "        train/<class_name>/*.jpg\n",
        "        val/<class_name>/*.jpg   (or valid/)\n",
        "        test/<class_name>/*.jpg\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_dir, batch_size=32, img_size=224, num_workers=2):\n",
        "        self.data_dir = data_dir\n",
        "        self.batch_size = batch_size\n",
        "        self.img_size = img_size\n",
        "        self.num_workers = num_workers\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((img_size, img_size)),\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "    def load_split(self, split):\n",
        "        split_dir = os.path.join(self.data_dir, split)\n",
        "        if not os.path.isdir(split_dir):\n",
        "            raise FileNotFoundError(f\"Directory not found: {split_dir}\")\n",
        "\n",
        "        dataset = datasets.ImageFolder(root=split_dir, transform=self.transform)\n",
        "\n",
        "        loader = DataLoader(\n",
        "            dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=(split == \"train\"),\n",
        "            num_workers=self.num_workers,\n",
        "            pin_memory=True,\n",
        "            persistent_workers=(self.num_workers > 0),\n",
        "            drop_last=(split == \"train\"),\n",
        "        )\n",
        "\n",
        "        return dataset, loader\n",
        "\n",
        "    def load_data(self):\n",
        "        data = {}\n",
        "\n",
        "        # allow \"val\" or \"valid\"\n",
        "        split_candidates = {\n",
        "            \"train\": [\"train\"],\n",
        "            \"val\": [\"val\", \"valid\"],\n",
        "            \"test\": [\"test\"],\n",
        "        }\n",
        "\n",
        "        for canonical, candidates in split_candidates.items():\n",
        "            loaded = False\n",
        "            for split in candidates:\n",
        "                try:\n",
        "                    dataset, loader = self.load_split(split)\n",
        "                    data[canonical] = {\n",
        "                        \"dataset\": dataset,\n",
        "                        \"loader\": loader,\n",
        "                        \"num_samples\": len(dataset),\n",
        "                        \"num_classes\": len(dataset.classes),\n",
        "                        \"classes\": dataset.classes,\n",
        "                        \"root\": os.path.join(self.data_dir, split),\n",
        "                    }\n",
        "                    loaded = True\n",
        "                    break\n",
        "                except FileNotFoundError:\n",
        "                    continue\n",
        "\n",
        "            if not loaded:\n",
        "                print(f\"Skipping missing split: {canonical} (looked for: {candidates})\")\n",
        "\n",
        "        if not data:\n",
        "            print(\n",
        "                \"\\nNo data found.\\n\"\n",
        "                \"Expected:\\n\"\n",
        "                f\"  {self.data_dir}/train/<class_name>/...\\n\"\n",
        "                f\"  {self.data_dir}/val(or valid)/<class_name>/...\\n\"\n",
        "                f\"  {self.data_dir}/test/<class_name>/...\\n\"\n",
        "            )\n",
        "\n",
        "        return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a1195d59",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1195d59",
        "outputId": "4c8f9a34-a872-4661-a2e1-669ea02284dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping missing split: train (looked for: ['train'])\n",
            "Skipping missing split: val (looked for: ['val', 'valid'])\n",
            "Skipping missing split: test (looked for: ['test'])\n",
            "\n",
            "No data found.\n",
            "Expected:\n",
            "  ./data/train/<class_name>/...\n",
            "  ./data/val(or valid)/<class_name>/...\n",
            "  ./data/test/<class_name>/...\n",
            "\n",
            "data_dict is empty (dataset not available yet).\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "data = Data(data_dir=\"./data\", batch_size=32, img_size=224, num_workers=2)\n",
        "data_dict = data.load_data()\n",
        "\n",
        "if not data_dict:\n",
        "    print(\"data_dict is empty (dataset not available yet).\")\n",
        "else:\n",
        "    for split, info in data_dict.items():\n",
        "        print(\n",
        "            f\"{split}: {info['num_samples']} samples | \"\n",
        "            f\"{info['num_classes']} classes\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a6fa112",
      "metadata": {
        "id": "0a6fa112"
      },
      "source": [
        "# 2 - Visualization\n",
        "This section provides basic visualization utilities to inspect the dataset.\n",
        "It helps verify that images and labels are loaded correctly before training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "1245b15e",
      "metadata": {
        "id": "1245b15e"
      },
      "outputs": [],
      "source": [
        "class Visualize:\n",
        "    def __init__(self, data_dict):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data_dict (dict): Output of Data.load_data()\n",
        "        \"\"\"\n",
        "        self.data_dict = data_dict\n",
        "\n",
        "    def plot_samples(self, split=\"train\", n_samples=5):\n",
        "        \"\"\"\n",
        "        Plot a few sample images from a given split.\n",
        "        \"\"\"\n",
        "        if split not in self.data_dict:\n",
        "            print(f\"Split '{split}' not found. Available: {list(self.data_dict.keys())}\")\n",
        "            return\n",
        "\n",
        "        dataset = self.data_dict[split][\"dataset\"]\n",
        "        n_samples = min(n_samples, len(dataset))\n",
        "\n",
        "        fig, axes = plt.subplots(1, n_samples, figsize=(15, 4))\n",
        "        if n_samples == 1:\n",
        "            axes = [axes]\n",
        "\n",
        "        for ax, idx in zip(axes, range(n_samples)):\n",
        "            img, label = dataset[idx]\n",
        "            ax.imshow(img)\n",
        "            ax.set_title(f\"Label: {label}\")\n",
        "            ax.axis(\"off\")\n",
        "\n",
        "        plt.suptitle(f\"{split.capitalize()} samples\")\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "44035d1f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44035d1f",
        "outputId": "f9e895dd-15bb-42bd-9a65-902119dc0770"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping missing split: train (looked for: ['train'])\n",
            "Skipping missing split: val (looked for: ['val', 'valid'])\n",
            "Skipping missing split: test (looked for: ['test'])\n",
            "\n",
            "No data found.\n",
            "Expected:\n",
            "  ./data/train/<class_name>/...\n",
            "  ./data/val(or valid)/<class_name>/...\n",
            "  ./data/test/<class_name>/...\n",
            "\n",
            "Skipping visualization: 'train' split not available.\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "data = Data(data_dir=\"./data\", batch_size=32)\n",
        "data_dict = data.load_data()\n",
        "\n",
        "if \"train\" in data_dict:\n",
        "    visualize = Visualize(data_dict)\n",
        "    visualize.plot_samples(split=\"train\", n_samples=5)\n",
        "else:\n",
        "    print(\"Skipping visualization: 'train' split not available.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba4a6c33",
      "metadata": {
        "id": "ba4a6c33"
      },
      "source": [
        "# 3 - Training\n",
        "This section implements a simple **baseline training pipeline**.\n",
        "It supports two modes:\n",
        "\n",
        "- **Dummy mode** (used when no dataset is available yet)\n",
        "- **Real data mode** (automatically activated once data is provided)\n",
        "\n",
        "The goal is to validate the end-to-end training and evaluation workflow."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Minimal CNN baseline for image classification.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(32 * 56 * 56, 128),  # assumes 224x224 input\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "0BYmi4WQMwT3"
      },
      "id": "0BYmi4WQMwT3",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "c7d94565",
      "metadata": {
        "id": "c7d94565"
      },
      "outputs": [],
      "source": [
        "class Train:\n",
        "    def __init__(self, data_dict, device=None):\n",
        "        self.data_dict = data_dict\n",
        "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model = None\n",
        "\n",
        "    def train_dummy(self, epochs=2):\n",
        "        print(\"Training in dummy mode (no real data found).\")\n",
        "\n",
        "        X = torch.randn(128, 3, 224, 224)\n",
        "        y = torch.randint(0, 5, (128,))\n",
        "\n",
        "        ds = TensorDataset(X, y)\n",
        "        dl = DataLoader(ds, batch_size=16, shuffle=True)\n",
        "\n",
        "        model = SimpleCNN(num_classes=5).to(self.device)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=3e-5, weight_decay=1e-4)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            model.train()\n",
        "            losses = []\n",
        "\n",
        "            for xb, yb in dl:\n",
        "                xb, yb = xb.to(self.device), yb.to(self.device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(xb)\n",
        "                loss = criterion(outputs, yb)\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                optimizer.step()\n",
        "                losses.append(loss.item())\n",
        "\n",
        "            print(\n",
        "                f\"Epoch {epoch+1}/{epochs} | \"\n",
        "                f\"Loss (mean): {np.mean(losses):.4f} | \"\n",
        "                f\"Loss (max): {np.max(losses):.4f}\"\n",
        "            )\n",
        "\n",
        "        self.model = model\n",
        "\n",
        "    def train_real(self, epochs=5):\n",
        "        # keep your real training code here (or leave placeholder)\n",
        "        print(\"Real data mode not implemented yet.\")\n",
        "        self.model = None\n",
        "\n",
        "    def train(self):\n",
        "        if \"train\" not in self.data_dict:\n",
        "            self.train_dummy()\n",
        "        else:\n",
        "            self.train_real()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "05f1324c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05f1324c",
        "outputId": "c8f0bdf0-0fbb-4ebf-983c-6f8ba96d80a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training in dummy mode (no real data found).\n",
            "Epoch 1/2 | Loss (mean): 1.6985 | Loss (max): 1.8643\n",
            "Epoch 2/2 | Loss (mean): 1.5838 | Loss (max): 1.6636\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "trainer = Train(data_dict)\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83e797f7",
      "metadata": {
        "id": "83e797f7"
      },
      "source": [
        "# 4 - Scoring\n",
        "This section evaluates a trained model using standard classification metrics.\n",
        "If validation or test data is not available yet, scoring is skipped gracefully.\n",
        "\n",
        "Once real data is provided, this section can be used to compare performance\n",
        "across domains and assess generalization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "56a01112",
      "metadata": {
        "id": "56a01112"
      },
      "outputs": [],
      "source": [
        "class Score:\n",
        "    def __init__(self, model, data_dict, device=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            model (nn.Module): trained model\n",
        "            data_dict (dict): output of Data.load_data()\n",
        "            device (str): 'cpu' or 'cuda'\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.data_dict = data_dict\n",
        "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.results = {}\n",
        "\n",
        "    def evaluate(self, split=\"val\"):\n",
        "        \"\"\"\n",
        "        Evaluate model on a given split.\n",
        "        \"\"\"\n",
        "        if split not in self.data_dict:\n",
        "            print(f\"Cannot evaluate: split '{split}' not available.\")\n",
        "            return None\n",
        "\n",
        "        loader = self.data_dict[split][\"loader\"]\n",
        "        self.model.eval()\n",
        "\n",
        "        y_true, y_pred = [], []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels in loader:\n",
        "                images = images.to(self.device)\n",
        "                outputs = self.model(images)\n",
        "                preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
        "\n",
        "                y_true.extend(labels.numpy())\n",
        "                y_pred.extend(preds)\n",
        "\n",
        "        acc = accuracy_score(y_true, y_pred)\n",
        "\n",
        "        self.results[split] = {\n",
        "            \"accuracy\": acc,\n",
        "            \"report\": classification_report(y_true, y_pred, output_dict=True)\n",
        "        }\n",
        "\n",
        "        print(f\"{split.capitalize()} Accuracy: {acc:.4f}\")\n",
        "        return self.results[split]\n",
        "\n",
        "    def summary(self):\n",
        "        \"\"\"\n",
        "        Print a summary of all computed scores.\n",
        "        \"\"\"\n",
        "        if not self.results:\n",
        "            print(\"No scores computed yet.\")\n",
        "            return\n",
        "\n",
        "        print(\"\\nScoring Summary\")\n",
        "        for split, res in self.results.items():\n",
        "            print(f\"- {split}: accuracy = {res['accuracy']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "48bcd23d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48bcd23d",
        "outputId": "c0229f36-b36a-4bdf-c686-4d8a979eb6f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scoring skipped: no 'val' or 'test' split found.\n",
            "Available splits: []\n",
            "Add data to ./data/val or ./data/test to enable scoring.\n",
            "No scores computed yet.\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "if trainer.model is None:\n",
        "    print(\"No trained model available for scoring yet.\")\n",
        "else:\n",
        "    scorer = Score(trainer.model, data_dict)\n",
        "\n",
        "    # Prefer val, else test\n",
        "    if \"val\" in data_dict:\n",
        "        scorer.evaluate(\"val\")\n",
        "    elif \"test\" in data_dict:\n",
        "        scorer.evaluate(\"test\")\n",
        "    else:\n",
        "        print(\n",
        "            \"Scoring skipped: no 'val' or 'test' split found.\\n\"\n",
        "            f\"Available splits: {list(data_dict.keys())}\\n\"\n",
        "            \"Add data to ./data/val or ./data/test to enable scoring.\"\n",
        "        )\n",
        "\n",
        "    scorer.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ede87bdd",
      "metadata": {
        "id": "ede87bdd"
      },
      "source": [
        "# 5 - (Optional) Prepare submission for Codabench\n",
        "This section prepares a submission ZIP compatible with Codabench.\n",
        "\n",
        "Depending on the competition setup, the submission can contain:\n",
        "- the trained **model checkpoint** (code submission), or\n",
        "- the **predictions file** (result submission).\n",
        "\n",
        "This is provided as a template and can be adapted once the final\n",
        "Codabench submission format is defined."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bccf7cb8-cf09-4e12-8129-2a0bd06b147c",
      "metadata": {
        "id": "bccf7cb8-cf09-4e12-8129-2a0bd06b147c"
      },
      "source": [
        "***\n",
        "\n",
        "In this section you should prepare a zip of the trained model (if your competition is a code submission competition) or zip of the predictions (if your competition is a result submission competition).\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "c08d8459",
      "metadata": {
        "id": "c08d8459"
      },
      "outputs": [],
      "source": [
        "class Submission:\n",
        "    \"\"\"\n",
        "    Creates a clean Codabench submission zip.\n",
        "\n",
        "    Best practice:\n",
        "    - Save artifacts into ./submission/\n",
        "    - Create the .zip OUTSIDE ./submission/ to avoid nesting old zips\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, submission_dir=\"./submission\", zip_file_name=None, clean_dir=True):\n",
        "        self.submission_dir = submission_dir\n",
        "        os.makedirs(self.submission_dir, exist_ok=True)\n",
        "\n",
        "        # Optional: clean the folder to avoid zipping older artifacts\n",
        "        if clean_dir:\n",
        "            for fn in os.listdir(self.submission_dir):\n",
        "                fp = os.path.join(self.submission_dir, fn)\n",
        "                if os.path.isfile(fp):\n",
        "                    os.remove(fp)\n",
        "\n",
        "        if zip_file_name is None:\n",
        "            zip_file_name = f\"Submission_{datetime.datetime.now().strftime('%y-%m-%d-%H-%M')}.zip\"\n",
        "        self.zip_file_name = zip_file_name\n",
        "\n",
        "        # IMPORTANT: zip path is outside submission_dir\n",
        "        self.zip_path = os.path.join(\".\", self.zip_file_name)\n",
        "\n",
        "    def write_readme(self):\n",
        "        readme_path = os.path.join(self.submission_dir, \"README.txt\")\n",
        "        with open(readme_path, \"w\") as f:\n",
        "            f.write(\n",
        "                \"Grain 1 – Generalization AI Challenge\\n\"\n",
        "                \"Submission generated by starting kit.\\n\"\n",
        "                \"Contains model checkpoint OR predictions.\\n\"\n",
        "            )\n",
        "        return readme_path\n",
        "\n",
        "    def save_code(self, model):\n",
        "        \"\"\"Save trained model checkpoint (code submission).\"\"\"\n",
        "        if model is None:\n",
        "            print(\"No trained model available to save.\")\n",
        "            return None\n",
        "\n",
        "        model_path = os.path.join(self.submission_dir, \"model.pth\")\n",
        "\n",
        "        # If it's a torch.nn.Module, save state_dict; otherwise try saving directly\n",
        "        if hasattr(model, \"state_dict\"):\n",
        "            torch.save(model.state_dict(), model_path)\n",
        "        else:\n",
        "            torch.save(model, model_path)\n",
        "\n",
        "        print(f\"Model saved at: {model_path}\")\n",
        "        return model_path\n",
        "\n",
        "    def save_result(self, predictions=None):\n",
        "        \"\"\"Save predictions file (result submission).\"\"\"\n",
        "        if predictions is None:\n",
        "            print(\"No predictions provided. Saving dummy predictions.\")\n",
        "            predictions = np.zeros(10, dtype=int)\n",
        "\n",
        "        pred_path = os.path.join(self.submission_dir, \"predictions.json\")\n",
        "        with open(pred_path, \"w\") as f:\n",
        "            json.dump({\"predictions\": list(map(int, predictions))}, f)\n",
        "\n",
        "        print(f\"Predictions saved at: {pred_path}\")\n",
        "        return pred_path\n",
        "\n",
        "    def zip_submission(self):\n",
        "        \"\"\"Create zip from submission_dir contents (no nested zip issue).\"\"\"\n",
        "        # Remove existing zip if present\n",
        "        if os.path.exists(self.zip_path):\n",
        "            os.remove(self.zip_path)\n",
        "\n",
        "        with zipfile.ZipFile(self.zip_path, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
        "            for filename in os.listdir(self.submission_dir):\n",
        "                file_path = os.path.join(self.submission_dir, filename)\n",
        "                if os.path.isfile(file_path):\n",
        "                    zf.write(file_path, arcname=filename)\n",
        "\n",
        "        size_mb = os.path.getsize(self.zip_path) / (1024 * 1024)\n",
        "        print(f\"Submission ZIP saved at: {self.zip_path} ({size_mb:.2f} MB)\")\n",
        "\n",
        "        # Quick integrity test\n",
        "        with zipfile.ZipFile(self.zip_path, \"r\") as zf:\n",
        "            ok = (zf.testzip() is None)\n",
        "            print(\"ZIP test:\", \"OK\" if ok else \"Corrupt\")\n",
        "            print(\"Files in ZIP:\", zf.namelist())\n",
        "\n",
        "        return self.zip_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "d2700348",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "id": "d2700348",
        "outputId": "e615bb3e-0923-4a90-b4b7-f6468b039d62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved at: ./submission/model.pth\n",
            "Submission ZIP saved at: ./Submission_26-01-10-02-04.zip (45.48 MB)\n",
            "ZIP test: OK\n",
            "Files in ZIP: ['model.pth', 'README.txt']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_2d84e2fb-feb4-4122-8e74-df8ecc633ebc\", \"Submission_26-01-10-02-04.zip\", 47684814)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Example usage\n",
        "submission = Submission(submission_dir=\"./submission\", clean_dir=True)\n",
        "submission.write_readme()\n",
        "submission.save_code(trainer.model)\n",
        "zip_path = submission.zip_submission()\n",
        "\n",
        "# Download in Colab\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download(zip_path)\n",
        "except Exception as e:\n",
        "    print(\"Not running in Colab or download failed:\", e)\n",
        "    print(\"ZIP is available at:\", zip_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FT1wWY7FtjlT"
      },
      "id": "FT1wWY7FtjlT",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}